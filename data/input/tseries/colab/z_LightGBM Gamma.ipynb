{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/girmdshinsei/for-japanese-beginner-with-wrmsse-in-lgbm\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom scipy.sparse import csr_matrix\nfrom scipy.stats import gamma\n\nfor dirname, _, filenames in os.walk('./input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading files...')\n    calendar = pd.read_csv('/kaggle/input/m5-forecasting-uncertainty/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    #\n    sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-uncertainty/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    #\n    sales_train_val = pd.read_csv('/kaggle/input/m5-forecasting-uncertainty/sales_train_validation.csv')\n    print(\n        'Sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0], sales_train_val.shape[1]))\n    #\n    submission = pd.read_csv('/kaggle/input/m5-forecasting-uncertainty/sample_submission.csv')\n    #\n    return calendar, sell_prices, sales_train_val, submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar, sell_prices, sales_train_val, submission = read_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_ITEMS = sales_train_val.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_categorical(df, cols):\n    for col in cols:\n        # Leave NaN as it is.\n        le = LabelEncoder()\n        not_null = df[col][df[col].notnull()]\n        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n    #\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = encode_categorical(\n    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n).pipe(reduce_mem_usage)\n\nsales_train_val = encode_categorical(\n    sales_train_val, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n).pipe(reduce_mem_usage)\n\nsell_prices = encode_categorical(sell_prices, [\"item_id\", \"store_id\"]).pipe(\n    reduce_mem_usage\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows = 365 * 1 * NUM_ITEMS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = pd.melt(sales_train_val,\n                                     id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n                                     var_name = 'day', value_name = 'demand')\nprint('Melted sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0],\n                                                                            sales_train_val.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = sales_train_val.iloc[-nrows:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate test dataframes\nforecast_submission = pd.concat([submission.iloc[0:int(771120/2),:].iloc[-30490:,:],submission.iloc[int(771120/2):,:].iloc[-30490:,:]])\nforecast_submission['id'] = forecast_submission['id'].str.replace('_.\\...._','_')\nforecast_submission.drop_duplicates(inplace=True)\n\n# submission fileのidのvalidation部分と, ealuation部分の名前を取得\ntest1_rows = [row for row in forecast_submission['id'] if 'validation' in row]\ntest2_rows = [row for row in forecast_submission['id'] if 'evaluation' in row]\n\n# submission fileのvalidation部分をtest1, ealuation部分をtest2として取得\ntest1 = forecast_submission[forecast_submission['id'].isin(test1_rows)]\ntest2 = forecast_submission[forecast_submission['id'].isin(test2_rows)]\n\n# test1, test2の列名の\"F_X\"の箇所をd_XXX\"の形式に変更\ntest1.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\ntest2.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n\n# test2のidの'_evaluation'を置換\n#test1['id'] = test1['id'].str.replace('_validation','')\ntest2['id'] = test2['id'].str.replace('_evaluation','_validation')\n\n# sales_train_valからidの詳細部分(itemやdepartmentなどのid)を重複なく一意に取得。\nproduct = sales_train_val[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n\n# idをキーにして, idの詳細部分をtest1, test2に結合する.\ntest1 = test1.merge(product, how = 'left', on = 'id')\ntest2 = test2.merge(product, how = 'left', on = 'id')\n\n# test1, test2をともにmelt処理する.（売上数量:demandは0）\ntest1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\ntest2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\n# validation部分と, evaluation部分がわかるようにpartという列を作り、 test1,test2のラベルを付ける。\nsales_train_val['part'] = 'train'\ntest1['part'] = 'test1'\ntest2['part'] = 'test2'\n\n# sales_train_valとtest1, test2の縦結合.\ndata = pd.concat([sales_train_val, test1, test2], axis = 0)\n\n# memoryの開放\ndel test1, test2, sales_train_val, forecast_submission\n\n# delete test2 for now(6/1以前は, validation部分のみ提出のため.)\ndata = data[data['part'] != 'test2']\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calendarの結合\n# drop some calendar features(不要な変数の削除:weekdayやwdayなどはdatetime変数から後ほど作成できる。)\ncalendar.drop(['weekday', 'wday', 'month', 'year'],\n              inplace = True, axis = 1)\n\n# notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)(dayとdをキーにdataに結合)\ndata = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\ndata.drop(['d', 'day'], inplace = True, axis = 1)\n\n# memoryの開放\ndel  calendar\ngc.collect()\n\n#sell priceの結合\n# get the sell price data (this feature should be very important)\ndata = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\nprint('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n\n# memoryの開放\ndel  sell_prices\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_fe(data):\n    # demand features(過去の数量から変数生成)\n    #\n    for diff in [0, 1, 2]:\n        shift = DAYS_PRED + diff\n        data[f\"shift_t{shift}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(shift)\n        )\n    #\n    for size in [7, 30, 60, 90, 180]:\n        data[f\"rolling_std_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(size).std()\n        )\n    #\n    for size in [7, 30, 60, 90, 180]:\n        data[f\"rolling_mean_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(size).mean()\n        )\n    #\n    data[\"rolling_skew_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).skew()\n    )\n    data[\"rolling_kurt_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).kurt()\n    )\n    #\n    # price features\n    # priceの動きと特徴量化（価格の変化率、過去1年間の最大価格との比など）\n    #\n    data[\"shift_price_t1\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1)\n    )\n    data[\"price_change_t1\"] = (data[\"shift_price_t1\"] - data[\"sell_price\"]) / (\n        data[\"shift_price_t1\"]\n    )\n    data[\"rolling_price_max_t365\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1).rolling(365).max()\n    )\n    data[\"price_change_t365\"] = (data[\"rolling_price_max_t365\"] - data[\"sell_price\"]) / (\n        data[\"rolling_price_max_t365\"]\n    )\n    #\n    data[\"rolling_price_std_t7\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(7).std()\n    )\n    data[\"rolling_price_std_t30\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(30).std()\n    )\n    #\n    # time features\n    # 日付に関するデータ\n    dt_col = \"date\"\n    data[dt_col] = pd.to_datetime(data[dt_col])\n    #\n    attrs = [\n        \"year\",\n        \"quarter\",\n        \"month\",\n        \"week\",\n        \"day\",\n        \"dayofweek\",\n        \"is_year_end\",\n        \"is_year_start\",\n        \"is_quarter_end\",\n        \"is_quarter_start\",\n        \"is_month_end\",\n        \"is_month_start\",\n    ]\n    #\n    for attr in attrs:\n        dtype = np.int16 if attr == \"year\" else np.int8\n        data[attr] = getattr(data[dt_col].dt, attr).astype(dtype)\n    #\n    data[\"is_weekend\"] = data[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n    #\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = simple_fe(data)\ndata = reduce_mem_usage(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# going to evaluate with the last 28 days\nx_train = data[data['date'] <= '2016-03-27']\ny_train = x_train['demand']\nx_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\ny_val = x_val['demand']\ntest = data[(data['date'] > '2016-04-24')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"event_name_1\",\n    \"event_type_1\",\n    \"event_name_2\",\n    \"event_type_2\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"sell_price\",\n    # demand features.\n    \"shift_t28\",\n    \"shift_t29\",\n    \"shift_t30\",\n    \"rolling_std_t7\",\n    \"rolling_std_t30\",\n    \"rolling_std_t60\",\n    \"rolling_std_t90\",\n    \"rolling_std_t180\",\n    \"rolling_mean_t7\",\n    \"rolling_mean_t30\",\n    \"rolling_mean_t60\",\n    \"rolling_mean_t90\",\n    \"rolling_mean_t180\",\n    \"rolling_skew_t30\",\n    \"rolling_kurt_t30\",\n    # price features\n    \"price_change_t1\",\n    \"price_change_t365\",\n    \"rolling_price_std_t7\",\n    \"rolling_price_std_t30\",\n    # time features.\n    \"year\",\n    \"month\",\n    \"week\",\n    \"day\",\n    \"dayofweek\",\n    \"is_year_end\",\n    \"is_year_start\",\n    \"is_quarter_end\",\n    \"is_quarter_start\",\n    \"is_month_end\",\n    \"is_month_start\",\n    \"is_weekend\"\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_mat = np.c_[np.identity(NUM_ITEMS).astype(np.int8),  # item :level 12\n                   np.ones([NUM_ITEMS, 1]).astype(np.int8),  # level 1\n                   pd.get_dummies(product.state_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.cat_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.dept_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str), drop_first=False).astype(\n                       'int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str), drop_first=False).astype(\n                       'int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str), drop_first=False).astype(\n                       'int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str), drop_first=False).astype(\n                       'int8').values,\n                   pd.get_dummies(product.item_id.astype(str), drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str), drop_first=False).astype(\n                       'int8').values\n].T\n\nweight_mat_csr = csr_matrix(weight_mat)\ndel weight_mat\ngc.collect()\n\ndef weight_calc(data):\n    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n    #\n    sales_train_val = pd.read_csv('/kaggle/input/m5-forecasting-uncertainty/sales_train_validation.csv')\n    #\n    d_name = ['d_' + str(i + 1) for i in range(1913)]\n    #\n    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n    #\n    # calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n    # 1-1914のdayの数列のうち, 売上が存在しない日を一旦0にし、0を9999に置換。そのうえでminimum numberを計算\n    df_tmp = ((sales_train_val > 0) * np.tile(np.arange(1, 1914), (weight_mat_csr.shape[0], 1)))\n    #\n    start_no = np.min(np.where(df_tmp == 0, 9999, df_tmp), axis=1) - 1\n    #\n    # denominator of RMSSE / RMSSEの分母\n    weight1 = np.sum((np.diff(sales_train_val, axis=1) ** 2), axis=1) / (1913 - start_no)\n    #\n    # calculate the sales amount for each item/level\n    df_tmp = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n    df_tmp['amount'] = df_tmp['demand'] * df_tmp['sell_price']\n    df_tmp = df_tmp.groupby(['id'])['amount'].apply(np.sum).values\n    #\n    weight2 = weight_mat_csr * df_tmp\n    #\n    weight2 = weight2 / np.sum(weight2)\n    #\n    del sales_train_val\n    gc.collect()\n    #\n    return weight1, weight2\n\n\nweight1, weight2 = weight_calc(data)\n\n\ndef wrmsse(preds, data):\n    # actual obserbed values / 正解ラベル\n    y_true = np.array(data.get_label())\n    #\n    # number of columns\n    num_col = len(y_true) // NUM_ITEMS\n    #\n    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n    #\n    x_name = ['pred_' + str(i) for i in range(num_col)]\n    x_name2 = [\"act_\" + str(i) for i in range(num_col)]\n    #\n    train = np.array(weight_mat_csr * np.c_[reshaped_preds, reshaped_true])\n    #\n    score = np.sum(\n        np.sqrt(\n            np.mean(\n                np.square(\n                    train[:, :num_col] - train[:, num_col:])\n                , axis=1) / weight1) * weight2)\n    #\n    return 'wrmsse', score, False\n\n\ndef wrmsse_simple(preds, data):\n    # actual obserbed values / 正解ラベル\n    y_true = np.array(data.get_label())\n    #\n    # number of columns\n    num_col = len(y_true) // NUM_ITEMS\n    #\n    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n    #\n    train = np.c_[reshaped_preds, reshaped_true]\n    #\n    weight2_2 = weight2[:NUM_ITEMS]\n    weight2_2 = weight2_2 / np.sum(weight2_2)\n    #\n    score = np.sum(\n        np.sqrt(\n            np.mean(\n                np.square(\n                    train[:, :num_col] - train[:, num_col:])\n                , axis=1) / weight1[:NUM_ITEMS]) * weight2_2)\n    #\n    return 'wrmsse', score, False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n    'metric': 'custom',\n    'objective': 'regression',\n    'n_jobs': -1,\n    'seed': 236,\n    'learning_rate': 0.1,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 10,\n    'colsample_bytree': 0.75}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = lgb.Dataset(x_train[features], y_train)\nval_set = lgb.Dataset(x_val[features], y_val)\n\ndel x_train, y_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50,\n                  valid_sets = [train_set, val_set], verbose_eval = 100, feval= wrmsse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions for the mean of the uncertainty distribution\ny_pred = model.predict(test[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# treat each prediction y as the estimated mean of a gamma distribution with mean y and scale 1\n# and use ppf to obtain the corresponding estimates of the percentiles\nids = test['id'].unique()\ntheta=1.0\ny_pred_q = pd.DataFrame.from_dict({'id': np.repeat(ids,28), \n                               'date': np.tile(np.arange(np.datetime64(\"2016-04-25\"),np.datetime64(\"2016-05-23\")),30490),\n                               '0_005': gamma.ppf(0.005, y_pred, theta),\n                               '0_025': gamma.ppf(0.025, y_pred, theta),\n                               '0_165': gamma.ppf(0.165, y_pred, theta),\n                               '0_250': gamma.ppf(0.25, y_pred, theta),\n                               '0_500': gamma.ppf(0.5, y_pred, theta),\n                               '0_750': gamma.ppf(0.75, y_pred, theta),\n                               '0_835': gamma.ppf(0.835, y_pred, theta),\n                               '0_975': gamma.ppf(0.975, y_pred, theta),\n                               '0_995': gamma.ppf(0.995, y_pred, theta)\n                            })\ny_pred_q['item_id'] = y_pred_q['id'].map(lambda x: '_'.join(x.split('_')[0:3]))\ny_pred_q['dept_id'] = y_pred_q['id'].map(lambda x: '_'.join(x.split('_')[0:2]))\ny_pred_q['cat_id'] = y_pred_q['id'].map(lambda x: '_'.join(x.split('_')[0:1]))\ny_pred_q['store_id'] = y_pred_q['id'].map(lambda x: '_'.join(x.split('_')[3:5]))\ny_pred_q['state_id'] = y_pred_q['id'].map(lambda x: '_'.join(x.split('_')[3:4]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def agg_series(preds12, q):\n    # preds12 contains 30490 series at level 12 aggregate these to get the other series\n    preds12['id'] = preds12['item_id'] + \"_\" + preds12['store_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n    # level 11: Unit sales of product x, aggregated for each State: 9,147\n    preds11 = preds12.groupby(['date','item_id','state_id'], as_index=False)[q].sum()\n    preds11['id'] = preds11['state_id'] + \"_\" + preds11['item_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n    # level 10: Unit sales of product x, aggregated for all stores/states: 3,049\n    preds10 = preds11.groupby(['date','item_id'], as_index=False)[q].sum()\n    preds10['id'] = preds10['item_id'] + \"_X_\" + q.replace('_','.') + \"_validation\"\n    # level 9: Unit sales of all products, aggregated for each store and department: 70\n    preds09 = preds12.groupby(['date', 'store_id', 'dept_id'], as_index=False)[q].sum()\n    preds09['id'] = preds09['store_id'] + \"_\" + preds09['dept_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n    # level 8: Unit sales of all products, aggregated for each store and category: 30\n    preds08 = preds12.groupby(['date', 'store_id', 'cat_id'], as_index=False)[q].sum()\n    preds08['id'] = preds08['store_id'] + \"_\" + preds08['cat_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n    # level 7: Unit sales of all products, aggregated for each State and department: 21\n    preds07 = preds12.groupby(['date', 'state_id', 'dept_id'], as_index=False)[q].sum()\n    preds07['id'] = preds07['state_id'] + \"_\" + preds07['dept_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n    # level 6: Unit sales of all products, aggregated for each State and category: 9\n    preds06 = preds12.groupby(['date', 'state_id', 'cat_id'], as_index=False)[q].sum()\n    preds06['id'] = preds06['state_id'] + \"_\" + preds06['cat_id'] + \"_\" + q.replace('_','.') + \"_validation\"\n    # level 5: Unit sales of all products, aggregated for each department: 7\n    preds05 = preds12.groupby(['date', 'dept_id'], as_index=False)[q].sum()\n    preds05['id'] = preds05['dept_id'] + \"_X_\" + q.replace('_','.') + \"_validation\"\n    # level 4: Unit sales of all products, aggregated for each category: 3\n    preds04 = preds12.groupby(['date', 'cat_id'], as_index=False)[q].sum()\n    preds04['id'] = preds04['cat_id'] + \"_X_\" + q.replace('_','.') + \"_validation\"\n    # level 3: Unit sales of all products, aggregated for each store: 10\n    preds03 = preds12.groupby(['date', 'store_id'], as_index=False)[q].sum()\n    preds03['id'] = preds03['store_id'] + \"_X_\" + q.replace('_','.') + \"_validation\"\n    # level 2: Unit sales of all products, aggregated for each State: 3\n    preds02 = preds12.groupby(['date', 'state_id'], as_index=False)[q].sum()\n    preds02['id'] = preds02['state_id'] + \"_X_\" + q.replace('_','.') + \"_validation\"\n    # level 1: Unit sales of all products, aggregated for all stores/states: 1\n    preds01 = preds12.groupby(['date'], as_index=False)[q].sum()\n    preds01['id'] = 'Total_X_' + q.replace('_','.') + '_validation'\n    preds = pd.concat([preds01, preds02, preds03,\n                                        preds04, preds05, preds06,\n                                        preds07, preds08, preds09,\n                                        preds10, preds11, preds12],\n                                       ignore_index=True)\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_0_005 = agg_series(y_pred_q,'0_005')[['date', 'id', '0_005']]\npredictions_0_005 = predictions_0_005.pivot(index = 'id', columns = 'date', values = '0_005').reset_index()\npredictions_0_005.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\npredictions_0_025 = agg_series(y_pred_q,'0_025')[['date', 'id', '0_025']]\npredictions_0_025 = predictions_0_025.pivot(index = 'id', columns = 'date', values = '0_025').reset_index()\npredictions_0_025.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\npredictions_0_165 = agg_series(y_pred_q,'0_165')[['date', 'id', '0_165']]\npredictions_0_165 = predictions_0_165.pivot(index = 'id', columns = 'date', values = '0_165').reset_index()\npredictions_0_165.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\npredictions_0_250 = agg_series(y_pred_q,'0_250')[['date', 'id', '0_250']]\npredictions_0_250 = predictions_0_250.pivot(index = 'id', columns = 'date', values = '0_250').reset_index()\npredictions_0_250.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\npredictions_0_500 = agg_series(y_pred_q,'0_500')[['date', 'id', '0_500']]\npredictions_0_500 = predictions_0_500.pivot(index = 'id', columns = 'date', values = '0_500').reset_index()\npredictions_0_500.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\npredictions_0_750 = agg_series(y_pred_q,'0_750')[['date', 'id', '0_750']]\npredictions_0_750 = predictions_0_750.pivot(index = 'id', columns = 'date', values = '0_750').reset_index()\npredictions_0_750.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\npredictions_0_835 = agg_series(y_pred_q,'0_835')[['date', 'id', '0_835']]\npredictions_0_835 = predictions_0_835.pivot(index = 'id', columns = 'date', values = '0_835').reset_index()\npredictions_0_835.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\npredictions_0_975 = agg_series(y_pred_q,'0_975')[['date', 'id', '0_975']]\npredictions_0_975 = predictions_0_975.pivot(index = 'id', columns = 'date', values = '0_975').reset_index()\npredictions_0_975.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\npredictions_0_995 = agg_series(y_pred_q,'0_995')[['date', 'id', '0_995']]\npredictions_0_995 = predictions_0_995.pivot(index = 'id', columns = 'date', values = '0_995').reset_index()\npredictions_0_995.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pd.concat([predictions_0_005, predictions_0_025, predictions_0_165, predictions_0_250, predictions_0_500\n    , predictions_0_750, predictions_0_835, predictions_0_975, predictions_0_995])\npredictions = pd.concat([predictions, predictions])\npredictions['id'][-385560:] = predictions['id'][-385560:].str.replace('_validation','_evaluation')\npredictions.fillna(0,inplace=True)\npredictions.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}